opener
User
FILES_PARAM
external
InvalidFileNameError
creates
float
formats
ReadBuffer
Raises
time
semantics
created
Increment
NamespaceInputReader
__params
_current_key_range
BlobKey
VERSION_IDS_PARAM
objects
opened
basestring
_buffer_size
shown
_get_query_spec
Defaults
utf32
Utility
few
App
Used
prefix
lines
each
bin
can_query
tuples
key_to_namespace
spec
MapReduce
Logs
mapper_sec
A
behavior
_app
ASC
InvalidArgumentError
_split_input_from_params
net
json_dict
_
tell
parse
member
f
classes
define
d
e
keys
a
n
o
query_operation_as_str
join
k
i
v
new
including
t
s
r
sequence
p
library
base
testing
allowed_keys
Helper
x
AbstractDatastoreInputReader
INCLUDE_INCOMPLETE_PARAM
KEY_RANGE_PARAM
sizes
fetching
root
entity
exception
distributed
apache
enumerate
has
software
invalid
file_format_parser
description
permissions
processing
Initialize
AS
append
b64encode
Cloud
filenames
current_key_range
processed
STRING_LENGTH
ndb
_record_reader
items
names
delimited
fewer
make
raw
update
val
OBJECT_NAMES_PARAM
MODULE_VERSIONS_PARAM
Each
As
following
name
zipfile
An
implied
Initializes
offset
law
_start_file_index
is_single_namespace
blob_sizes
assigned
injection
finished
equal
Error
UnknownError
__all__
never
input_reader
expected
which
source
retrieve
there
Serializes
Whether
end_file_index
BadValueError
reason
buffer_size
Operator
otherwise
num_shards
errors
COUNT
given
byte
among
ctx
empty
itself
listed
element
end_index
support
job
MR
_split_input_from_namespace
Required
newline
http
will
mapped
filters
parseGlob
JsonMixin
_bucket
mr_input_readers
key_range
mapper
express
ValueError
Nones
property_range
unless
supported
encoding
OF
query_spec
hierarchy
create_from_ns_range
greatly
to_yield
prototype_request
No
OR
_OldAbstractDatastoreInputReader
DatastoreKeyInputReader
hasattr
per
method
get_short_name
warning
p_range
apply
single
ranges
_properties
contains
difficulty
since
every
type
ext
END_TIME_PARAM
NAMESPACE_PARAM
python
Supports
_PROTOTYPE_REQUEST_PARAM
request
listbucket
useful
required
namespace_key
raw_entity_kind
ds_query
argument
_to_key_ranges_by_shard
Must
IS
Assuming
namespaces
classpath
blobinfo
whole
binary_record
minimum_log_level
If
util
Optional
_filestream
start_position
writing
NamespaceRange
Combiner
It
Bad
super
paths
allows
corresponding
path
blob_keys
msec
KeyRangeEntityIterator
zip
your
into
params
interface
input_spec_shards
proto
_account_id
docstring
Delimiter
bucket_name
implementation
_read
KEYS_ONLY_PARAM
Decodes
also
advance
from_json
delegates
feeds
Model
expand_parameters
KeyRangesFactory
pop
its
Property
omitted
_bucket_iter
validate
shard_count
although
boolean
entirely
iterates
copy
cls
file_size
contents
To
KeyRange
_index
under
ImportError
model_class_path
always
expanded
log
entities
calls
__init__
_ns_range
read_buffer_size
Encode
specify
low
entity_type
_get_kind
sort
_key_ranges
Entities
fails
archives
BUCKET_NAME_PARAM
states
Defines
use
should_shard_by_property_range
Apache
Construct
mixes
best
usr
decodes
whatever
dumps
_end_position
supplied
seek
num
either
start_index
shared_ranges
end_position
property_name_as_str
allow_old
MapperSpec
datastore
for_name
must
namespace_end
_start_index
values
shards
populate
Allow
Internal
_has_iterated
include_start
loads
agreed
Value
whose
string_length
hundreds
Also
datastore_range_iterators
BlobstoreLineInputReader
all_filenames
Entries
p_ranges
shard_filenames
syntax
KeyRangeKeyIterator
ENTITY_KIND_PARAM
subdicationary
cast
current_key
_BLOB_BUFFER_SIZE
blob_chunk_size
KeyRanges
listdir
teaches
got
number_of_shards
attempt
valid
generate
can
file_service_pb
governing
www
_BATCH_SIZE
value_of_certain_type
kwargs
OFFSET_PARAM
END_INDEX_PARAM
Example
json
TODO
MAX_NAMESPACES_FOR_KEY_SHARD
serializable
multiple
_STUB
sorted
__name__
extended
you
_blob_reader
See
shards_per_blob
designate
ordered
usage
actual
InputReader
latest
mismatch
assign
tuple
ns_range
zero
Obtain
After
understands
subfiles
thousands
ParseFromString
size_per_shard
equals
extra
random
batch_list
like
__dict__
INITIAL_POSITION_PARAM
start_file_index
non
many
start
__iter__
current_values
some
DatastoreInputReader
_reader
outside
Use
frozenset
Unix
file_stat
bytes
range
xrange
block
line
NAMESPACES_PARAM
_choose_split_points
Pre
This
serialized
twice
RecordsReader
BlobstoreInputReaders
Copyright
starting
reversed
end
options
db_iters
module
slash
BlobInfo
entity_kind
oversampling_factor
prop
_read_before_start
env
len
stored
delimiter
process
Next
License
shuffle
Abstract
allowed
but
reading
FORMATS
infolist
Could
create_key_ranges_iterator
fileinfo
All
key_start
used
been
function
namespace
dependency
LICENSE
handler_for_name
longer
LogReadRequest
BATCH_SIZE_PARAM
metadata
bucket3
bucket2
BlobstoreZipLineInputReader
status
Power
Iterates
sorted_keys
logging
NotImplementedError
format
file_formats
Only
bucket1
COUNTER_IO_READ_BYTES
Storage
_OFFSET_PARAM
They
pickle
representation
batch
count_per_shard
_validate_filters_ndb
match
output
blobs
_get_raw_entity_kind
ExistenceError
issue
state
currently
encoded
entity_kind_or_model_classpath
because
API
subdictionary
equality
details
callable
_filters
making
resumed
Python
exist
combiner_result
FileFormatRoot
_blob_key
check
ANY
namespace_range_json
TOC
namespace_range
START_INDEX_PARAM
around
Non
blob
appengine
_do_validate
instead
uses
_STRING_MAX_FILES_LISTED
started
significantly
user
first
_iter
need
using
next_file_index
supports
Concrete
split_input
until
missing
lexographically
over
_GoogleCloudStorageOutputWriter
mapper_spec
ns_keys
subclassed
BlobstoreZipInputReader
ns_ranges
detected
Skipping
patterns
izable
number
subclasses
_end_file_index
requested
Filter
Index
bfiles
isinstance
COUNTER_IO_READ_MSEC
gs
OBJECT_NAME_PARAM
separator
Inherit
compute_split_points
exclusive
namespace_start
instances
could
query_operator_as_str
Create
map
current
entries
KeyValues
left
warnings
BadCombinerOutputError
parseable
readable
PropertyRange
may
readline
MINIMUM_LOG_LEVEL_PARAM
_entries
GoogleCloudStorageInputReader
Inc
FileInputReader
InputReaders
gain
starts
Split
what
RetrySliceError
Get
configuration
classmethod
include_end
returned
Unless
yet
blob_files
body
FailJobError
_filenames
_bucket_itr
least
However
docs
Reader
by
splitting
Primary
same
enough
completion
value_list
close
__namespace__
describes
Records
cloudstorage
future
optional
any
Encodes
KeyRangeModelIterator
__getstate__
key
be
db
keywords
get
strictly
end_time
initial_position
blob_info
new_pos
containing
Return
example
combiner_spec
Multiple
issubclass
make_datastore_query
Entity
set
Query
existing
index_stride
Otherwise
lineinfo
Returns
expects
k_range
BLOB_KEYS_PARAM
KIND
shard_start_indexes
all
You
yields
__scatter__
ids
at
_KEY_RANGE_ITER_CLS
num_files
message
base64
see
model_classpath
cascade
Fetches
an
compatability
they
ns
START_TIME_PARAM
configured
ranges_by_shard
no
_GoogleCloudStorageRecordInputReader
determine
of
operation
Files
_entity_kind
b64decode
only
on
create_from_list
_next_offset
key_ranges_by_ns
LevelDB
RequestLog
limit
fetch
keys_only
version_ids
Entry
op
_get_params
file1
_cur_handle
include
_delimiter
input_shard_state
DELIMITER_PARAM
_end_index
them
then
records
_APP_PARAM
caller
creation
raised
key_ranges
BlobstoreInputReader
most
WITHOUT
Engine
mapreduce
objectname
params_diff
remaining
sharding
DEFAULT_STRING_LENGTH
CONDITIONS
internal
current_key_range_json
files
JSON
key_ranges_json
batch_size
my
appearing
Keys
differ
_file_format_root
specified
additional
model_class
nd
random_keys
obj
within
archive
specifies
iter
EOFError
MetaModel
produce
staticmethod
Blobstore
LogInputReader
count
second
last
being
CloudStorage
buffer
include_incomplete
more
level
earliest
results
when
value
END_POSITION_PARAM
CURRENT_KEY_RANGE_PARAM
sets
Matching
_next_file
returning
Missing
iterator
appropriate
such
Thus
namespace_keys
int
FORMAT_PARAM
pieces
matching
called
ALLOW_CHECKPOINT
included
this
_GoogleCloudStorageInputReader
BLOB_KEY_PARAM
limitations
way
includes
applicable
rstrip
prefixes
ZipInfo
id
StringIO
Can
RangeIteratorFactory
May
read
treated
deepcopy
constructor
application
those
it
blob_key
delimiters
extend
gets
Tries
list
readers
io
to_json
_count
_MAX_SHARD_COUNT
removed
necessary
themselves
logservice
found
DatastoreEntityInputReader
handle
Key
passed
_validate_filters
determines
counters
key_end
dot
reduced
include_app_logs
START_FILE_INDEX_PARAM
EOF
points
_MAX_BLOB_KEYS_COUNT
chunks
turn
_iter_key_ranges
ends
module_versions
defined
how
There
iteritems
__class__
content
reads
NAMESPACE_RANGE_PARAM
delegating
split
Instantiates
blobkey
choice
bucket
next
data
Works
logged
spanning
ascii_lowercase
resulting
StopIteration
The
Splits
_ReducerReader
Input
reverse
_iter_ns_range
later
implemented
Note
cannot
blobstore
up
namespace_result
_OVERSAMPLING_FACTOR
pattern
seconds
string
combiner
documentation
doc
part
namespace_ranges
keep
to
both
round
Filename
SDK
after
takes
with_start_after
splits
st
Invalid
parameters
instance
Version
Forwards
FILTERS_PARAM
index
shallow
so
generates
_split_ns_by_scatter
Validate
providing
decode_data
size
that
input
reader
key_ranges_by_shard
reader_spec
associated
str
generated
than
Read
deserialized
_string_length
existent
result
model
zip_input
interpret
sum
strings
well
mapreduce_spec
iterate
consume
namespace_query
RandomStringInputReader
complete
SHARDS_PARAM
compatible
contiguous
supply
above
format_root
implement
is_generator
too
separate
ns_key
AbstractKeyRangeIterator
byteoffset
unicode
seconds_per_shard
_zip
py
Google
serves
out
from_json_object
via
QuerySpec
Due
BlobReader
unzips
parsedName
lexicographically
First
prototype
log_service_pb
ZipFile
open
repr
are
directories
_KWARGS
file
k_ranges
_iter_key_range
compression
Too
information
plus
exhausted
form
file_format_roots
create
without
concrete
RawDatastoreInputReader
present
record
obtain
inputs
BufferedFile
_initial_offset
app_id
language
_FileFormatParser
ProtocolBufferDecodeError
INCLUDE_APP_LOGS_PARAM
position
might
input_reader_class
google
NotFoundError
pair
query
Expected
org
kind
effective
mode
version
character
before_iter
total_size
length
padded
api
Iterate
One
giving
Object
Please
covered
min
BadReaderParamsError
Mapper
app
BUFFER_SIZE_PARAM
responsible
logs
current_shard_size
evenly
dictionary
should
Validates
valuable
scatter
entity_kind_name
deduplication
directory
Namespace
END_FILE_INDEX_PARAM
Constructor
based
Parameters
Operation
endswith
FILE_PARAM
compliance
get_namespace_keys
requests
json_util
ProtocolBuffer
yielded
returns
positive
encode
_ACCOUNT_ID_PARAM
create_property_range_iterator
account_id
Find
property
properties
split_query
WARRANTIES
TypeError
default
blob_size
_JSON_PICKLE
contain
even
filter
to_json_object
zfiles
stop
filenumber
dict
other
input_spec
specific
have
direction
one
suffixed
iters
Order
contained
licenses
object
Creates
validate_bucket_name
Args
shard
parameter
dir
_batch_size
attempts
Licensed
For
format_string
perfect
find
file_format_root
error
File
epoch
Yields
the
__str__
BASIS
_PARAMS
ending
skipped
entry
applied
specification
context
start_time
filename
bool
encode_data
