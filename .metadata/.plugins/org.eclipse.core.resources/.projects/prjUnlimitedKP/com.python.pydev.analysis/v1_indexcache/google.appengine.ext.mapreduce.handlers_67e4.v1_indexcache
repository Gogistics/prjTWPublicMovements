_process_inputs
float
MapreduceState
Raises
time
ShardContext
created
Enqueues
Recover
timed
Aborting
Defaults
from_request
unset
prefix
each
_CONTROLLER_PERIOD_SEC
bin
MapReduce
spec
Logs
unique
HugeTaskHandler
A
TaskAlreadyExistsError
_app
comparing
accordingly
PROCEED_TASK
Schedule
_
readers_json_str
outputs
X
mrshard
ignore
g
d
e
keys
controller_callback_task
a
n
i
new
t
s
r
base
Helper
_try_free_lease
physical
drop
mapper_output_writer
exception
entity
distributed
apache
enumerate
has
software
exc_clear
permissions
processing
writer_state
had
AS
hook
append
serialized_input_readers_key
execute
code
serial_id
ndb
names
CALLBACK_MR_ID_TASK_HEADER
mapper_output_writer_class
active
Command
update
Each
RESULT_SUCCESS
datastore_errors
response
_finalize_outputs
MapreduceControl
especially
_put_state
callbacks
name
implied
An
law
job_config
updates
Transaction
task_retry_count
injection
finished
Looks
Error
POST
entire
input_reader
never
flush
which
Issuing
take
adding
Whether
_get_mapreduce_spec
Method
reason
otherwise
errors
caches
enqueue_worker_task
ctx
_supports_slice_recovery
itself
increment
support
job
MR
http
Will
will
controller_callback
_TASK_DIRECTIVE
base_handler
inplitting
mapper
HugeTask
express
datum
clear
normally
clean
OF
unknown
Requirements
No
finalize_job
OR
Slice
request_id
hasattr
enqueue_kickoff_task
mapreduce_name
method
warning
output_writers
headers
single
get_by_job_id
contains
every
type
ext
python
exceeds
Context
slice_start_time
request
Callback
proceed
writes
required
writer
TASK_MAX_ATTEMPTS
cycle
fatal
enum
IS
timedelta
requires
id_or_name
mapper_input_reader
shard_description
If
util
retry_task
Optional
writing
Is
It
_wait_time
super
_set_ndb_cache_policy
rate
allows
path
AppEngine
zip
create_datastore_write_config
actively
added
validation
task
executor
into
modify
transactionally
params
counters_map
MapReduceState
interface
CleanUpJobHandler
schedule
get_hooks
implementation
mapreduce_state
consistent
modules
also
eta
Model
expand_parameters
FinalizeJobHandler
checks
its
validate
retried
shard_count
deleting
mapper_input_reader_class
permanently
recover_slice
_state_to_task
RESULT_ABORTED
mapper_output_writer_spec
validates
supplies
copy
cls
scanning
_get_task_headers
UI
under
ImportError
always
entities
log
calls
__init__
Processing
_maintain_LC
REQUEST_LOG_ID
Expect
unreachable
unspecified
active_shards
apiproxy_errors
validator_parameter
set_for_abort
idempotent
fails
set_for_success
states
Defines
use
Possible
run
Apache
decoding
RETRY_SHARD
life
We
Drop
secs
written
worker
usr
sahrd
correctness
transactional
slice_retry
utcfromtimestamp
dumps
paylod
supplied
medium
math
either
arguments
to_dict
retries
writers
Generate
enqueue_controller_task
Upon
MapperSpec
become
begin_slice
perform
Output
ended
for_name
datastore
result_status
timezone
shard_states
values
shards
ControllerCallbackHandler
care
hooks_class_name
Set
_check_mr_state
loads
agreed
control
schedules
saved
behind
randrange
storing
url
failure
Writing
retry_shard
saves
case
got
identifier
immediately
succeeded
attempt
valid
Exception
gracefully
can
governing
www
So
continuously
json
output_writer_class
invokes
get_all
mr_params
_STUB
helper
regular
__name__
processing_rate
you
See
TransientShardState
_to_map_job_config
InputReader
core
scan
ups
Save
PostJsonHandler
tuple
serialized_readers_entity
zero
info
mapreduce_params
PROCESSING_RATE_PER_SEC
_attempt_slice_retry
CURRENT_MODULE_ID
gathering
startswith
fail_task
slice_id
random
again
like
_get_input_readers
now
stopped
start
Makes
Use
mapper_handler
put
remove
validator
SHARD_COUNT
This
according
base_path
exceptions
serialized
slice_ctx
traceback
delete
_MAX_LEASE_DURATION_SEC
Copyright
end
just
TombstonedTaskError
env
already
len
NAMED
shard_number
_attempt_slice_recovery
process
JobContext
License
failed_shards
successfully
mapper_handler_spec
find_all_by_mapreduce_state
Handler
fresh_shard_state
Handles
Rollback
allowed
but
transaction
job_context
from_json_str
DROP_TASK
drop_task
All
set_for_failure
used
been
together
function
SHARD_MAX_ATTEMPTS
LICENSE
dependency
sent
json_response
Precondition
func
init_job
_time
sure
user_params
Dropping
status
get_queue_name
logging
NotImplementedError
mark
fresh_state
Only
Gracefully
enqueue_done_task
output
slice_processing_limit
last_slice
start_map
call
Any
TASK_MAX_DATA_PROCESSING_ATTEMPTS
_add_kickoff_task
_create_and_save_state
temporary
state
output_writer_spec
Got
because
API
end_slice
aborted
get_task_name
Abort
shard_context
beginning
ANY
done_callback
tasks
around
Job
appengine
mrcontrol
dropping
special
uses
incr
started
their
user
kicks
store
wait
continued
split_input
until
missing
shard_id
free
queue_name
over
mapper_spec
propagation
isolated
_SERIALIZED_INPUT_READERS_KEY
cleaned
parent
number
sleep
blocking
independently
input_readers
isinstance
_update_state_from_shard_states
Failed
shard_ctx
before
instances
PARAM_DONE_CALLBACK_QUEUE
execution
attempted
naive
modification
reset_for_retry
sync
current
write
map
datetime
safe
may
Finalize
max
web
Inc
exceptional
ABORT_SHARD
InputReaders
recovered
starts
exists
shard_state
successful
get_by_key_name
Get
Found
classmethod
returned
ago
Unless
yet
task_params
FailJobError
last_work_item
args
careful
normal
invocation
by
aware
same
Absolute
committing
cloudstorage
would
RETRY_SLICE
future
kickoffjob_callback
any
worker_task
key
be
forced
db
total_seconds
get
_get_required_param
_schedule_slice
Last
lease
containing
lightweight
force_writes
HTTP_X_APPENGINE_QUEUENAME
issubclass
COUNTER_MAPPER_WALLTIME_MS
does
Mapreduce
set
add_map
COUNTER_MAPPER_CALLS
Otherwise
Returns
SliceContext
QueueName
Run
KIND
all
You
mapper_params
ahead
params_validator
at
Always
still
see
an
assumption
off
done_task
badly
Supervises
they
_processing_limit
no
_attempt_shard_retry
advance_for_next_slice
_drop_gracefully
NotEnoughArgumentsError
operation
of
_recover
writer_class
transient_shard_state
on
only
new_mapreduce_id
acquring
worker_active_state_collision
limit
keys_only
fetch
os
_get_params
retry_slice
done
exceeded
to_json_str
them
finalizejob_callback
then
taskqueue
delay
calculate_keys_by_mapreduce_state
raised
finalize_task
param_name
Taskqueue
WITHOUT
recover
mapreduce
respected
CONDITIONS
_MR_ID_TASK_HEADER
Future
is_this_a_retry
mr
Datastore
flight
specified
additional
callback
Wait
obj
retry
iter
staticmethod
save
count
last
being
abort_shard
INDEPENDENT
handlers
_run_task_hook
_setup_output_writer
when
Process
value
sets
create_new
returning
tstate
iterator
Thus
int
Other
environ
describing
statement
_ShardLifeCycle
_txn
called
ALLOW_CHECKPOINT
controller
this
shard_id_from_number
hooks
slice_context
done_callback_method
runtime
limitations
applicable
proceed_task
add
was
id
Can
kickoff
_are_states_consistent
May
read
stuck
those
it
extend
important
worst
gets
serialized_input_readers
readers
list
to_json
duration
get_by_shard_id
adjusted_key
necessary
logservice
Compute
found
mutate
_clean_up_mr
handle
don
_has_old_request_ended
DeadlineExceededError
map_job
NOT
finalize
points
happen
place
piece
going
module_versions
duplicates
TaskQueueHandler
getattr
FAIL_TASK
available
_set
__class__
When
reads
request_ids
dropped
Task
shard_keys
existing_shard_states
split
we
_supports_shard_retry
set_processed_counts
common
sys
ceil
next
data
logged
JobConfig
running
StopIteration
The
critical
_try_acquire_lease
caught
_get_countdown_for_next_slice
_add_task
in_xg_transaction
_TEST_INJECTED_FAULTS
previous
later
RESULT_FAILED
slice_request_id
implemented
Note
eventually
reasonable
RECOVER_SLICE
up
seconds
string
doc
slice
recovery_slice
abort
to
_schedule_shards
queue
kick
key_name
HTTP
mapper_params_validator
_start_time
acquired_once
KickOffJobHandler
handled
after
_enum
commit
MANDATORY
splits
ss
parameters
Version
instance
_get_default_mr_params
so
Call
Validate
transient
RETRY_TASK
format_exc
logic
Release
that
input
reader
str
previously
Read
result
model
Update
catch
signal
get_key_by_job_id
_start_map
existing_shard_keys
input_readers_for_mr_
Shard
mapreduce_spec
_save_state_and_schedule_next
PARAM_DONE_CALLBACK
MapperWorkerCallbackHandler
ancestor
is_generator
_MAX_STATES_IN_MEMORY
_tx
end_shard
mr_id
reschedule
needed
recovery
State
invoke
Google
serial
out
handler
name_prefix
examing
via
payload
whether
_LEASE_DURATION_SEC
begin_shard
repr
aborted_shards
are
Under
fail
__return
initial
Even
file
Returned
get_current_version_name
config
_process_datum
output_writer
form
Waiting
delta
prepare
without
create
exit
_SLICE_DURATION_SEC
OutputWriter
obtain
Controller
inputs
ABORT
Combining
app_id
controller_parameters
language
Retrieves
AbortJobHandler
input_reader_class
google
subroutines
failed
encountered
Request
Contention
task_directive
org
worker_callback
mode
_finalize_job
Time
Hooks
command
acquired
api
split_param
want
hence
app
expire
Mapper
slice_retries
get_key_by_shard_id
responsible
logs
StartJobHandler
total_shards
should
mapper_input_reader_spec
Attempt
point
processing_limit
kickoff_task
Constructor
shard_life_cycle
based
simplejson
Parameters
old
Operation
dies
Try
compliance
though
anyway
mapreduce_id
Invokes
_save_states
Final
finished_shard
puts
dedicated
processed_counts
yielded
returns
last_poll_time
positive
mr_state
ShardState
spurious
task_name
WARRANTIES
countdown
default
shard_attempts
ever
stop
other
allow
specific
have
MapreduceSpec
licenses
_HugeTaskPayload
Handle
resolved_validator
Args
parameter
shard
attempts
die
Licensed
Fill
copy_from
Check
error
_set_state
naturally
lock
the
BASIS
_MR_SHARD_ID_TASK_HEADER
entry
specification
context
Prepares
bool
